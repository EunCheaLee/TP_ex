import json
import re
from typing import Dict, List
from collections import defaultdict
import pandas as pd
import os


class SentenceExtractor:
    def __init__(self):
        self.sentences_by_word = defaultdict(list)

        # soynlp ë¡œë“œ
        try:
            from soynlp.normalizer import repeat_normalize
            self.repeat_normalize = repeat_normalize
            print("âœ“ soynlp ë¡œë“œ ì™„ë£Œ")
        except ImportError:
            self.repeat_normalize = None
            print("âš  soynlp ë¡œë“œ ì‹¤íŒ¨ (ê¸°ë³¸ ë¡œì§ ì‚¬ìš©)")

        # í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸ ëª©ë¡
        self.particles = [
            'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ',
            'ë¶€í„°', 'ê¹Œì§€', 'ë¡œ', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë”ëŸ¬', 'ë¼ê³ ',
            'ê³ ', 'ë©°', 'ë©´', 'ì', 'ë‹ˆ', 'ëƒ', 'ë‚˜', 'ì§€', 'ìš”', 'ìŠµë‹ˆë‹¤', 'ã…‚ë‹ˆë‹¤'
        ]

    def normalize_text(self, text: str) -> str:
        """soynlpë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if self.repeat_normalize:
            text = self.repeat_normalize(text, num_repeats=2)
        return text

    def fix_spacing_comprehensive(self, text: str) -> str:
        """ê°•í™”ëœ ë„ì–´ì“°ê¸° ìˆ˜ì •"""
        text = self.normalize_text(text)

        def merge_single_chars(text):
            result = []
            parts = text.split()
            i = 0
            while i < len(parts):
                if i + 2 < len(parts) and all(len(parts[j]) == 1 for j in range(i, i + 3)):
                    merged = parts[i]
                    j = i + 1
                    while j < len(parts) and len(parts[j]) == 1:
                        merged += parts[j]
                        j += 1
                    result.append(merged)
                    i = j
                else:
                    result.append(parts[i])
                    i += 1
            return ' '.join(result)

        text = merge_single_chars(text)

        for particle in self.particles:
            text = re.sub(f'([ê°€-í£])\s+({particle})(?=\s|$|[,.!?])', r'\1\2', text)

        text = re.sub(r'([ê°€-í£]+)\s+(ì˜)\s+([ê°€-í£]+)', r'\1\2 \3', text)
        text = re.sub(r'([ê°€-í£]{1,3})\s+(ìŠµë‹ˆë‹¤|ã…‚ë‹ˆë‹¤|ì—ˆìŠµë‹ˆë‹¤|ì˜€ìŠµë‹ˆë‹¤|ê² ìŠµë‹ˆë‹¤)', r'\1\2', text)
        text = re.sub(r'([ê°€-í£]{1,3})\s+(í•©ë‹ˆë‹¤|í–ˆìŠµë‹ˆë‹¤|í•˜ë‹¤|í•˜ëŠ”|í•œ)', r'\1\2', text)
        text = re.sub(r'\s+([,.!?])', r'\1', text)
        text = re.sub(r'([,.!?])(?=[ê°€-í£])', r'\1 ', text)
        text = re.sub(r'(\d+)\s+(ê°œ|ëª…|ë§ˆë¦¬|ê¶Œ|ë²ˆ|ì‚´|ë…„|ì›”|ì¼)', r'\1\2', text)
        text = re.sub(r'\s+', ' ', text)

        return text.strip()

    def clean_sentence(self, sentence: str) -> str:
        """ë¬¸ì¥ ì •ì œ"""
        sentence = re.sub(r'[^\w\s\.\!\?\,]', '', sentence)
        sentence = re.sub(r'[0-9ï¼Œã€‚ã€]', '', sentence)
        sentence = re.sub(r'\s+', ' ', sentence)
        return sentence.strip()

    def is_valid_sentence(self, sentence: str) -> bool:
        """ìœ íš¨í•œ ë¬¸ì¥ì¸ì§€ ê²€ì‚¬"""
        if not (10 <= len(sentence) <= 100):
            return False

        korean_chars = len(re.findall(r'[ê°€-í£]', sentence))
        if korean_chars < len(sentence) * 0.5:
            return False

        if re.search(r'([ê°€-í£])\1{3,}', sentence):
            return False

        words = sentence.split()
        if words and len(words) > 3 and all(len(w) <= 2 for w in words):
            return False

        return True

    def split_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë¶„ë¦¬"""
        sentences = re.split(r'[\.!?]+', text)

        cleaned = []
        for sent in sentences:
            sent = self.clean_sentence(sent)
            if self.is_valid_sentence(sent):
                cleaned.append(sent)

        return cleaned

    def extract_sentences_with_word(self, text: str, target_word: str) -> List[str]:
        """íŠ¹ì • ë‹¨ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ"""
        sentences = self.split_sentences(text)

        matching = []
        for sent in sentences:
            if target_word in sent:
                matching.append(sent)

        return matching

    def build_sentence_database(self, fairytales: Dict[str, str], vocabulary_df: pd.DataFrame):
        """ëª¨ë“  ë™í™”ì—ì„œ ê° ë‹¨ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ"""
        print("\n" + "=" * 60)
        print("ğŸ“ ë™í™” ì›ë¬¸ì—ì„œ ë¬¸ì¥ ì¶”ì¶œ ì¤‘...")
        print("=" * 60)

        total_words = len(vocabulary_df)
        found_count = 0

        for idx, row in vocabulary_df.iterrows():
            word = row['word']

            if (idx + 1) % 100 == 0:
                print(f"  ì§„í–‰: {idx + 1}/{total_words} ({(idx + 1) / total_words * 100:.1f}%)")

            for title, content in fairytales.items():
                sentences = self.extract_sentences_with_word(content, word)
                if sentences:
                    self.sentences_by_word[word].extend(sentences)
                    found_count += 1

        for word in self.sentences_by_word:
            unique_sents = list(set(self.sentences_by_word[word]))
            unique_sents.sort(key=len)
            self.sentences_by_word[word] = unique_sents[:10]

        words_with_sentences = sum(1 for sents in self.sentences_by_word.values() if sents)
        total_sentences = sum(len(sents) for sents in self.sentences_by_word.values())

        print(f"\nâœ… ë¬¸ì¥ ì¶”ì¶œ ì™„ë£Œ!")
        print(f"  ğŸ“Š ë¬¸ì¥ì´ ìˆëŠ” ë‹¨ì–´: {words_with_sentences}/{total_words}ê°œ ({words_with_sentences / total_words * 100:.1f}%)")
        print(f"  ğŸ“ ì´ ì¶”ì¶œëœ ë¬¸ì¥: {total_sentences:,}ê°œ")
        if words_with_sentences > 0:
            print(f"  ğŸ“ˆ í‰ê·  ë¬¸ì¥/ë‹¨ì–´: {total_sentences / words_with_sentences:.1f}ê°œ\n")

        return self.sentences_by_word

    def save_to_json(self, filepath: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        data = {word: sents for word, sents in self.sentences_by_word.items() if sents}

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ… ë¬¸ì¥ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥: {filepath}")
        print(f"   ë‹¨ì–´ ìˆ˜: {len(data):,}ê°œ\n")


def load_fairytales(filepath: str, source: str) -> Dict[str, str]:
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)

    fairytales = {}

    if source == "grimm":
        stories = data.get("fairy_tales", [])
        for s in stories:
            key = f"{s.get('number','')}. {s.get('title','')}"
            fairytales[key] = s.get("content","")

    elif source == "aesops":
        stories = data.get("fables", [])
        for s in stories:
            key = f"{s.get('id','')}. {s.get('title','')}" if s.get("id") else s.get("title","")
            fairytales[key] = s.get("content","")

    return fairytales

# ì‹¤í–‰
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸ”§ ë™í™” í•™ìŠµ íŒŒì´í”„ë¼ì¸ (ê·¸ë¦¼ í˜•ì œ í¬í•¨)")
    print("=" * 60)

    # JSON íŒŒì¼ ê²½ë¡œ
    json_files = {
        'hwp': '../data/json/cleaned_hwp.json',
        'pdf': '../data/json/cleaned_pdf.json',
        'grimm': '../data/json/grim_bro.json',  # ê·¸ë¦¼ í˜•ì œ ë™í™”
        'aesops': '../data/json/aesop_fables.json',
    }

    # 1. JSON ë¡œë“œ
    fairytales = {}
    print("\nğŸ“‚ ë™í™” íŒŒì¼ ë¡œë“œ ì¤‘...")

    # HWP, PDF (ê¸°ì¡´ í˜•ì‹)
    for key in ['hwp', 'pdf']:
        filepath = json_files[key]
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                fairytales.update(data)
            print(f"  âœ“ {key.upper()}: {len(data)}ê¶Œ")
        except FileNotFoundError:
            print(f"  âš  {key.upper()}: íŒŒì¼ ì—†ìŒ ({filepath})")
        except Exception as e:
            print(f"  âŒ {key.upper()}: {e}")

    # ê·¸ë¦¼ í˜•ì œ ë™í™” (ìƒˆë¡œìš´ í˜•ì‹)
    grimm_file = json_files['grimm']
    try:
        grimm_tales = load_fairytales(json_files['grimm'], "grimm")
        fairytales.update(grimm_tales)
        print(f"  âœ“ ê·¸ë¦¼ í˜•ì œ: {len(grimm_tales)}ê¶Œ")
    except FileNotFoundError:
        print(f"  âš  ê·¸ë¦¼ í˜•ì œ: íŒŒì¼ ì—†ìŒ ({grimm_file})")
    except Exception as e:
        print(f"  âŒ ê·¸ë¦¼ í˜•ì œ: {e}")

    if not fairytales:
        print("\nâŒ ë¡œë“œëœ ë™í™”ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print("íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\n")
        exit(1)

    # ì´ì† ìš°í™” ë™í™” (ìƒˆë¡œìš´ í˜•ì‹)
    aesops_file = json_files['aesops']
    try:
        aesops_tales = load_fairytales(json_files['aesops'], "aesops")
        fairytales.update(aesops_tales)
        print(f"  âœ“ ì´ì† ìš°í™”: {len(aesops_tales)}ê¶Œ")
    except FileNotFoundError:
        print(f"  âš  ì´ì† ìš°í™”: íŒŒì¼ ì—†ìŒ ({aesops_file})")
    except Exception as e:
        print(f"  âŒ ì´ì† ìš°í™”: {e}")

    if not fairytales:
        print("\nâŒ ë¡œë“œëœ ë™í™”ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print("íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\n")
        exit(1)

    print(f"\nâœ… ì´ {len(fairytales)}ê¶Œ ë¡œë“œ\n")

    # 2. ë„ì–´ì“°ê¸° êµì •
    extractor = SentenceExtractor()

    print("=" * 60)
    print("ğŸ”§ ë„ì–´ì“°ê¸° êµì • ì¤‘ (soynlp)...")
    print("=" * 60)

    fixed_fairytales = {}
    for idx, (title, content) in enumerate(fairytales.items(), 1):
        display_title = title[:50] + "..." if len(title) > 50 else title
        print(f"  [{idx}/{len(fairytales)}] {display_title}")

        fixed_content = extractor.fix_spacing_comprehensive(content)
        fixed_fairytales[title] = fixed_content

    # 3. ì €ì¥
    output_file = '../data/json/fixed_fairytales.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(fixed_fairytales, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ë„ì–´ì“°ê¸° êµì • ì™„ë£Œ: {output_file}\n")

    # 4. ì–´íœ˜ ë°ì´í„° ë¡œë“œ
    vocab_file = 'vocabulary_data_reclassified.csv'

    if not os.path.exists(vocab_file):
        print(f"âŒ {vocab_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € ì–´íœ˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n")
        exit(1)

    vocab_df = pd.read_csv(vocab_file)
    print(f"âœ“ ì–´íœ˜ ë°ì´í„°: {len(vocab_df):,}ê°œ ë‹¨ì–´\n")

    # 5. ë¬¸ì¥ ì¶”ì¶œ
    sentences_db = extractor.build_sentence_database(fixed_fairytales, vocab_df)

    # 6. ì €ì¥
    extractor.save_to_json('sentences_database.json')

    # 7. ìƒ˜í”Œ
    print("=" * 60)
    print("ğŸ“‹ ìƒ˜í”Œ í™•ì¸")
    print("=" * 60)

    shown = 0
    for word, sentences in sentences_db.items():
        if sentences and shown < 5:
            print(f"\nğŸ’¬ '{word}' ({len(sentences)}ê°œ ë¬¸ì¥)")
            for i, sent in enumerate(sentences[:2], 1):
                print(f"   {i}. {sent}")
            shown += 1

    print("\n" + "=" * 60)
    print("âœ… ì™„ë£Œ!")
    print("=" * 60)
    print("\nğŸ“¦ ìƒì„± íŒŒì¼:")
    print(f"  âœ“ {output_file}")
    print(f"  âœ“ sentences_database.json")
    print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
    print(f"  ì „ì²´ ë™í™”: {len(fairytales)}ê¶Œ")
    print(f"  ì¶”ì¶œ ë¬¸ì¥: {sum(len(s) for s in sentences_db.values()):,}ê°œ")
    print(f"  í•™ìŠµ ë‹¨ì–´: {len([w for w, s in sentences_db.items() if s])}ê°œ")
    print("\nğŸš€ FastAPI ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”!\n")