# pip install transformers[sentencepiece] accelerate
import os
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# =========================
# 1. ëª¨ë¸/í† í¬ë‚˜ì´ì € ì •ë³´
# =========================
model_name = "sionic-ai/nllb-200-ko-gec-600M"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
offload_dir = "./offload"
os.makedirs(offload_dir, exist_ok=True)

# =========================
# 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
# =========================
print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    offload_folder=offload_dir
)

model.to(device)
model.eval()
print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
print(f"ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” ì¥ì¹˜: {device}")  # ğŸ”¹ GPU/CPU í™•ì¸

# =========================
# 3. ì…ë ¥/ì¶œë ¥ íŒŒì¼
# =========================
input_path = "../data/txt/sentences_spaced_pykospacing.txt"
output_path = "../data/txt/sentences_corrected_kogec.txt"

# =========================
# 4. í•œ ë²ˆì— ë°°ì¹˜ ì²˜ë¦¬ (GPU ë©”ëª¨ë¦¬ ìµœì í™”)
# =========================
# VRAM ì—¬ìœ ì— ë§ì¶° ë°°ì¹˜ í¬ê¸° ê³„ì‚°
if device.type == "cuda":
    # ì˜ˆì‹œ: GTX 1660 ê¸°ì¤€, ì¤„ ìˆ˜ì™€ ê¸¸ì´ ê³ ë ¤
    batch_size = 16
else:
    batch_size = 1  # CPUëŠ” í•œ ë²ˆì— í•˜ë‚˜ì”©

# ì…ë ¥ ë¬¸ì¥ ëª¨ë‘ ì½ê¸°
with open(input_path, "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f if line.strip()]

output_lines = []

for i in range(0, len(lines), batch_size):
    batch = lines[i:i+batch_size]

    # í† í°í™” + GPU ì´ë™
    inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=False
        )

    # ë””ì½”ë”© í›„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    for out in outputs:
        corrected = tokenizer.decode(out, skip_special_tokens=True)
        output_lines.append(corrected)

    print(f"[{i + len(batch)}] lines processed...")

# ê²°ê³¼ ì €ì¥
with open(output_path, "w", encoding="utf-8") as f_out:
    f_out.write("\n".join(output_lines))

print("ëª¨ë“  ë¬¸ì¥ êµì • ì™„ë£Œ! ê²°ê³¼ëŠ”", output_path, "ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
